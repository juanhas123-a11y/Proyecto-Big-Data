{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c993761-42bb-4a31-97b5-2bac76b28640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cassandra-driver\n",
      "  Using cached cassandra_driver-3.29.3-cp313-cp313-win_amd64.whl.metadata (7.0 kB)\n",
      "Collecting clickhouse-driver\n",
      "  Using cached clickhouse_driver-0.2.10-cp313-cp313-win_amd64.whl.metadata (6.6 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-3.0.0-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Collecting pyspark\n",
      "  Using cached pyspark-4.1.1.tar.gz (455.4 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting geomet>=1.1 (from cassandra-driver)\n",
      "  Using cached geomet-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pytz (from clickhouse-driver)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzlocal (from clickhouse-driver)\n",
      "  Using cached tzlocal-5.3.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-2.4.1-cp313-cp313-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\juanh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\juanh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.3)\n",
      "Collecting py4j<0.10.9.10,>=0.10.9.7 (from pyspark)\n",
      "  Using cached py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting click (from geomet>=1.1->cassandra-driver)\n",
      "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\juanh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\juanh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from click->geomet>=1.1->cassandra-driver) (0.4.6)\n",
      "Using cached cassandra_driver-3.29.3-cp313-cp313-win_amd64.whl (349 kB)\n",
      "Using cached clickhouse_driver-0.2.10-cp313-cp313-win_amd64.whl (203 kB)\n",
      "Using cached pandas-3.0.0-cp313-cp313-win_amd64.whl (9.7 MB)\n",
      "Using cached py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "Using cached geomet-1.1.0-py3-none-any.whl (31 kB)\n",
      "Using cached numpy-2.4.1-cp313-cp313-win_amd64.whl (12.3 MB)\n",
      "Using cached click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (pyproject.toml): started\n",
      "  Building wheel for pyspark (pyproject.toml): finished with status 'error'\n",
      "Failed to build pyspark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for pyspark (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [1925 lines of output]\n",
      "  running bdist_wheel\n",
      "  C:\\Users\\juanh\\AppData\\Local\\Temp\\pip-build-env-zbpcbxp0\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\cmd.py:135: SetuptoolsDeprecationWarning: bdist_wheel.universal is deprecated\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          With Python 2.7 end-of-life, support for building universal wheels\n",
      "          (i.e., wheels that support both Python 2 and Python 3)\n",
      "          is being obviated.\n",
      "          Please discontinue using this option, or if you still need it,\n",
      "          file an issue with pypa/setuptools describing your use case.\n",
      "  \n",
      "          This deprecation is overdue, please update your project and remove deprecated\n",
      "          calls to avoid build errors in the future.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    self.finalize_options()\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\\lib\\pyspark\n",
      "  copying pyspark\\accumulators.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\conf.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\daemon.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\errors_doc_gen.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\find_spark_home.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\install.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\instrumentation_utils.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\java_gateway.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\join.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\loose_version.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\profiler.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\rddsampler.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\resultiterable.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\serializers.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\shell.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\shuffle.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\statcounter.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\storagelevel.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\taskcontext.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\traceback_utils.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\util.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\version.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\worker.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\worker_util.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\_globals.py -> build\\lib\\pyspark\n",
      "  copying pyspark\\__init__.py -> build\\lib\\pyspark\n",
      "  creating build\\lib\\pyspark\\core\n",
      "  copying pyspark\\core\\broadcast.py -> build\\lib\\pyspark\\core\n",
      "  copying pyspark\\core\\context.py -> build\\lib\\pyspark\\core\n",
      "  copying pyspark\\core\\files.py -> build\\lib\\pyspark\\core\n",
      "  copying pyspark\\core\\rdd.py -> build\\lib\\pyspark\\core\n",
      "  copying pyspark\\core\\status.py -> build\\lib\\pyspark\\core\n",
      "  copying pyspark\\core\\__init__.py -> build\\lib\\pyspark\\core\n",
      "  creating build\\lib\\pyspark\\cloudpickle\n",
      "  copying pyspark\\cloudpickle\\cloudpickle.py -> build\\lib\\pyspark\\cloudpickle\n",
      "  copying pyspark\\cloudpickle\\cloudpickle_fast.py -> build\\lib\\pyspark\\cloudpickle\n",
      "  copying pyspark\\cloudpickle\\__init__.py -> build\\lib\\pyspark\\cloudpickle\n",
      "  creating build\\lib\\pyspark\\mllib\n",
      "  copying pyspark\\mllib\\classification.py -> build\\lib\\pyspark\\mllib\n",
      "  copying pyspark\\mllib\\clustering.py -> build\\lib\\pyspark\\mllib\n",
      "  copying pyspark\\mllib\\common.py -> build\\lib\\pyspark\\mllib\n",
      "  copying pyspark\\mllib\\evaluation.py -> build\\lib\\pyspark\\mllib\n",
      "  copying pyspark\\mllib\\feature.py -> build\\lib\\pyspark\\mllib\n",
      "  copying pyspark\\mllib\\fpm.py -> build\\lib\\pyspark\\mllib\n",
      "  copying pyspark\\mllib\\random.py -> build\\lib\\pyspark\\mllib\n",
      "  copying pyspark\\mllib\\recommendation.py -> build\\lib\\pyspark\\mllib\n",
      "  copying pyspark\\mllib\\regression.py -> build\\lib\\pyspark\\mllib\n",
      "  copying pyspark\\mllib\\tree.py -> build\\lib\\pyspark\\mllib\n",
      "  copying pyspark\\mllib\\util.py -> build\\lib\\pyspark\\mllib\n",
      "  copying pyspark\\mllib\\__init__.py -> build\\lib\\pyspark\\mllib\n",
      "  creating build\\lib\\pyspark\\mllib\\linalg\n",
      "  copying pyspark\\mllib\\linalg\\distributed.py -> build\\lib\\pyspark\\mllib\\linalg\n",
      "  copying pyspark\\mllib\\linalg\\__init__.py -> build\\lib\\pyspark\\mllib\\linalg\n",
      "  creating build\\lib\\pyspark\\mllib\\stat\n",
      "  copying pyspark\\mllib\\stat\\distribution.py -> build\\lib\\pyspark\\mllib\\stat\n",
      "  copying pyspark\\mllib\\stat\\KernelDensity.py -> build\\lib\\pyspark\\mllib\\stat\n",
      "  copying pyspark\\mllib\\stat\\test.py -> build\\lib\\pyspark\\mllib\\stat\n",
      "  copying pyspark\\mllib\\stat\\_statistics.py -> build\\lib\\pyspark\\mllib\\stat\n",
      "  copying pyspark\\mllib\\stat\\__init__.py -> build\\lib\\pyspark\\mllib\\stat\n",
      "  creating build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\base.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\classification.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\clustering.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\common.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\dl_util.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\evaluation.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\feature.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\fpm.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\functions.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\image.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\model_cache.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\pipeline.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\recommendation.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\regression.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\stat.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\tree.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\tuning.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\util.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\wrapper.py -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\ml\\__init__.py -> build\\lib\\pyspark\\ml\n",
      "  creating build\\lib\\pyspark\\ml\\connect\n",
      "  copying pyspark\\ml\\connect\\base.py -> build\\lib\\pyspark\\ml\\connect\n",
      "  copying pyspark\\ml\\connect\\classification.py -> build\\lib\\pyspark\\ml\\connect\n",
      "  copying pyspark\\ml\\connect\\evaluation.py -> build\\lib\\pyspark\\ml\\connect\n",
      "  copying pyspark\\ml\\connect\\feature.py -> build\\lib\\pyspark\\ml\\connect\n",
      "  copying pyspark\\ml\\connect\\functions.py -> build\\lib\\pyspark\\ml\\connect\n",
      "  copying pyspark\\ml\\connect\\io_utils.py -> build\\lib\\pyspark\\ml\\connect\n",
      "  copying pyspark\\ml\\connect\\pipeline.py -> build\\lib\\pyspark\\ml\\connect\n",
      "  copying pyspark\\ml\\connect\\proto.py -> build\\lib\\pyspark\\ml\\connect\n",
      "  copying pyspark\\ml\\connect\\readwrite.py -> build\\lib\\pyspark\\ml\\connect\n",
      "  copying pyspark\\ml\\connect\\serialize.py -> build\\lib\\pyspark\\ml\\connect\n",
      "  copying pyspark\\ml\\connect\\summarizer.py -> build\\lib\\pyspark\\ml\\connect\n",
      "  copying pyspark\\ml\\connect\\tuning.py -> build\\lib\\pyspark\\ml\\connect\n",
      "  copying pyspark\\ml\\connect\\util.py -> build\\lib\\pyspark\\ml\\connect\n",
      "  copying pyspark\\ml\\connect\\__init__.py -> build\\lib\\pyspark\\ml\\connect\n",
      "  creating build\\lib\\pyspark\\ml\\linalg\n",
      "  copying pyspark\\ml\\linalg\\__init__.py -> build\\lib\\pyspark\\ml\\linalg\n",
      "  creating build\\lib\\pyspark\\ml\\param\n",
      "  copying pyspark\\ml\\param\\shared.py -> build\\lib\\pyspark\\ml\\param\n",
      "  copying pyspark\\ml\\param\\_shared_params_code_gen.py -> build\\lib\\pyspark\\ml\\param\n",
      "  copying pyspark\\ml\\param\\__init__.py -> build\\lib\\pyspark\\ml\\param\n",
      "  creating build\\lib\\pyspark\\ml\\torch\n",
      "  copying pyspark\\ml\\torch\\data.py -> build\\lib\\pyspark\\ml\\torch\n",
      "  copying pyspark\\ml\\torch\\distributor.py -> build\\lib\\pyspark\\ml\\torch\n",
      "  copying pyspark\\ml\\torch\\log_communication.py -> build\\lib\\pyspark\\ml\\torch\n",
      "  copying pyspark\\ml\\torch\\torch_run_process_wrapper.py -> build\\lib\\pyspark\\ml\\torch\n",
      "  copying pyspark\\ml\\torch\\__init__.py -> build\\lib\\pyspark\\ml\\torch\n",
      "  creating build\\lib\\pyspark\\ml\\deepspeed\n",
      "  copying pyspark\\ml\\deepspeed\\deepspeed_distributor.py -> build\\lib\\pyspark\\ml\\deepspeed\n",
      "  copying pyspark\\ml\\deepspeed\\__init__.py -> build\\lib\\pyspark\\ml\\deepspeed\n",
      "  creating build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\catalog.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\column.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\conf.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\context.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\conversion.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\dataframe.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\datasource.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\datasource_internal.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\geo_utils.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\group.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\internal.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\merge.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\metrics.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\observation.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\profiler.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\readwriter.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\session.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\sql_formatter.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\table_arg.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\tvf.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\tvf_argument.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\types.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\udf.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\udtf.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\utils.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\variant_utils.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\window.py -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\__init__.py -> build\\lib\\pyspark\\sql\n",
      "  creating build\\lib\\pyspark\\sql\\avro\n",
      "  copying pyspark\\sql\\avro\\functions.py -> build\\lib\\pyspark\\sql\\avro\n",
      "  copying pyspark\\sql\\avro\\__init__.py -> build\\lib\\pyspark\\sql\\avro\n",
      "  creating build\\lib\\pyspark\\sql\\classic\n",
      "  copying pyspark\\sql\\classic\\column.py -> build\\lib\\pyspark\\sql\\classic\n",
      "  copying pyspark\\sql\\classic\\dataframe.py -> build\\lib\\pyspark\\sql\\classic\n",
      "  copying pyspark\\sql\\classic\\table_arg.py -> build\\lib\\pyspark\\sql\\classic\n",
      "  copying pyspark\\sql\\classic\\window.py -> build\\lib\\pyspark\\sql\\classic\n",
      "  copying pyspark\\sql\\classic\\__init__.py -> build\\lib\\pyspark\\sql\\classic\n",
      "  creating build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\catalog.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\column.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\conf.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\conversion.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\dataframe.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\datasource.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\expressions.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\group.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\logging.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\merge.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\observation.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\plan.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\profiler.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\readwriter.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\session.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\sql_formatter.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\table_arg.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\tvf.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\types.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\udf.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\udtf.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\utils.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\window.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\_typing.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  copying pyspark\\sql\\connect\\__init__.py -> build\\lib\\pyspark\\sql\\connect\n",
      "  creating build\\lib\\pyspark\\sql\\connect\\avro\n",
      "  copying pyspark\\sql\\connect\\avro\\functions.py -> build\\lib\\pyspark\\sql\\connect\\avro\n",
      "  copying pyspark\\sql\\connect\\avro\\__init__.py -> build\\lib\\pyspark\\sql\\connect\\avro\n",
      "  creating build\\lib\\pyspark\\sql\\connect\\client\n",
      "  copying pyspark\\sql\\connect\\client\\artifact.py -> build\\lib\\pyspark\\sql\\connect\\client\n",
      "  copying pyspark\\sql\\connect\\client\\core.py -> build\\lib\\pyspark\\sql\\connect\\client\n",
      "  copying pyspark\\sql\\connect\\client\\reattach.py -> build\\lib\\pyspark\\sql\\connect\\client\n",
      "  copying pyspark\\sql\\connect\\client\\retries.py -> build\\lib\\pyspark\\sql\\connect\\client\n",
      "  copying pyspark\\sql\\connect\\client\\__init__.py -> build\\lib\\pyspark\\sql\\connect\\client\n",
      "  creating build\\lib\\pyspark\\sql\\connect\\functions\n",
      "  copying pyspark\\sql\\connect\\functions\\builtin.py -> build\\lib\\pyspark\\sql\\connect\\functions\n",
      "  copying pyspark\\sql\\connect\\functions\\partitioning.py -> build\\lib\\pyspark\\sql\\connect\\functions\n",
      "  copying pyspark\\sql\\connect\\functions\\__init__.py -> build\\lib\\pyspark\\sql\\connect\\functions\n",
      "  creating build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\base_pb2.py -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\base_pb2_grpc.py -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\catalog_pb2.py -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\commands_pb2.py -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\common_pb2.py -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\example_plugins_pb2.py -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\expressions_pb2.py -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\ml_common_pb2.py -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\ml_pb2.py -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\pipelines_pb2.py -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\relations_pb2.py -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\types_pb2.py -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\__init__.py -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  creating build\\lib\\pyspark\\sql\\connect\\protobuf\n",
      "  copying pyspark\\sql\\connect\\protobuf\\functions.py -> build\\lib\\pyspark\\sql\\connect\\protobuf\n",
      "  copying pyspark\\sql\\connect\\protobuf\\__init__.py -> build\\lib\\pyspark\\sql\\connect\\protobuf\n",
      "  creating build\\lib\\pyspark\\sql\\connect\\resource\n",
      "  copying pyspark\\sql\\connect\\resource\\profile.py -> build\\lib\\pyspark\\sql\\connect\\resource\n",
      "  copying pyspark\\sql\\connect\\resource\\__init__.py -> build\\lib\\pyspark\\sql\\connect\\resource\n",
      "  creating build\\lib\\pyspark\\sql\\connect\\shell\n",
      "  copying pyspark\\sql\\connect\\shell\\progress.py -> build\\lib\\pyspark\\sql\\connect\\shell\n",
      "  copying pyspark\\sql\\connect\\shell\\__init__.py -> build\\lib\\pyspark\\sql\\connect\\shell\n",
      "  creating build\\lib\\pyspark\\sql\\connect\\streaming\n",
      "  copying pyspark\\sql\\connect\\streaming\\query.py -> build\\lib\\pyspark\\sql\\connect\\streaming\n",
      "  copying pyspark\\sql\\connect\\streaming\\readwriter.py -> build\\lib\\pyspark\\sql\\connect\\streaming\n",
      "  copying pyspark\\sql\\connect\\streaming\\__init__.py -> build\\lib\\pyspark\\sql\\connect\\streaming\n",
      "  creating build\\lib\\pyspark\\sql\\connect\\streaming\\worker\n",
      "  copying pyspark\\sql\\connect\\streaming\\worker\\foreach_batch_worker.py -> build\\lib\\pyspark\\sql\\connect\\streaming\\worker\n",
      "  copying pyspark\\sql\\connect\\streaming\\worker\\listener_worker.py -> build\\lib\\pyspark\\sql\\connect\\streaming\\worker\n",
      "  copying pyspark\\sql\\connect\\streaming\\worker\\__init__.py -> build\\lib\\pyspark\\sql\\connect\\streaming\\worker\n",
      "  creating build\\lib\\pyspark\\sql\\functions\n",
      "  copying pyspark\\sql\\functions\\builtin.py -> build\\lib\\pyspark\\sql\\functions\n",
      "  copying pyspark\\sql\\functions\\partitioning.py -> build\\lib\\pyspark\\sql\\functions\n",
      "  copying pyspark\\sql\\functions\\__init__.py -> build\\lib\\pyspark\\sql\\functions\n",
      "  creating build\\lib\\pyspark\\sql\\pandas\n",
      "  copying pyspark\\sql\\pandas\\conversion.py -> build\\lib\\pyspark\\sql\\pandas\n",
      "  copying pyspark\\sql\\pandas\\functions.py -> build\\lib\\pyspark\\sql\\pandas\n",
      "  copying pyspark\\sql\\pandas\\group_ops.py -> build\\lib\\pyspark\\sql\\pandas\n",
      "  copying pyspark\\sql\\pandas\\map_ops.py -> build\\lib\\pyspark\\sql\\pandas\n",
      "  copying pyspark\\sql\\pandas\\serializers.py -> build\\lib\\pyspark\\sql\\pandas\n",
      "  copying pyspark\\sql\\pandas\\typehints.py -> build\\lib\\pyspark\\sql\\pandas\n",
      "  copying pyspark\\sql\\pandas\\types.py -> build\\lib\\pyspark\\sql\\pandas\n",
      "  copying pyspark\\sql\\pandas\\utils.py -> build\\lib\\pyspark\\sql\\pandas\n",
      "  copying pyspark\\sql\\pandas\\__init__.py -> build\\lib\\pyspark\\sql\\pandas\n",
      "  creating build\\lib\\pyspark\\sql\\plot\n",
      "  copying pyspark\\sql\\plot\\core.py -> build\\lib\\pyspark\\sql\\plot\n",
      "  copying pyspark\\sql\\plot\\plotly.py -> build\\lib\\pyspark\\sql\\plot\n",
      "  copying pyspark\\sql\\plot\\__init__.py -> build\\lib\\pyspark\\sql\\plot\n",
      "  creating build\\lib\\pyspark\\sql\\protobuf\n",
      "  copying pyspark\\sql\\protobuf\\functions.py -> build\\lib\\pyspark\\sql\\protobuf\n",
      "  copying pyspark\\sql\\protobuf\\__init__.py -> build\\lib\\pyspark\\sql\\protobuf\n",
      "  creating build\\lib\\pyspark\\sql\\streaming\n",
      "  copying pyspark\\sql\\streaming\\listener.py -> build\\lib\\pyspark\\sql\\streaming\n",
      "  copying pyspark\\sql\\streaming\\list_state_client.py -> build\\lib\\pyspark\\sql\\streaming\n",
      "  copying pyspark\\sql\\streaming\\map_state_client.py -> build\\lib\\pyspark\\sql\\streaming\n",
      "  copying pyspark\\sql\\streaming\\python_streaming_source_runner.py -> build\\lib\\pyspark\\sql\\streaming\n",
      "  copying pyspark\\sql\\streaming\\query.py -> build\\lib\\pyspark\\sql\\streaming\n",
      "  copying pyspark\\sql\\streaming\\readwriter.py -> build\\lib\\pyspark\\sql\\streaming\n",
      "  copying pyspark\\sql\\streaming\\state.py -> build\\lib\\pyspark\\sql\\streaming\n",
      "  copying pyspark\\sql\\streaming\\stateful_processor.py -> build\\lib\\pyspark\\sql\\streaming\n",
      "  copying pyspark\\sql\\streaming\\stateful_processor_api_client.py -> build\\lib\\pyspark\\sql\\streaming\n",
      "  copying pyspark\\sql\\streaming\\stateful_processor_util.py -> build\\lib\\pyspark\\sql\\streaming\n",
      "  copying pyspark\\sql\\streaming\\transform_with_state_driver_worker.py -> build\\lib\\pyspark\\sql\\streaming\n",
      "  copying pyspark\\sql\\streaming\\value_state_client.py -> build\\lib\\pyspark\\sql\\streaming\n",
      "  copying pyspark\\sql\\streaming\\__init__.py -> build\\lib\\pyspark\\sql\\streaming\n",
      "  creating build\\lib\\pyspark\\sql\\streaming\\proto\n",
      "  copying pyspark\\sql\\streaming\\proto\\StateMessage_pb2.py -> build\\lib\\pyspark\\sql\\streaming\\proto\n",
      "  copying pyspark\\sql\\streaming\\proto\\__init__.py -> build\\lib\\pyspark\\sql\\streaming\\proto\n",
      "  creating build\\lib\\pyspark\\sql\\worker\n",
      "  copying pyspark\\sql\\worker\\analyze_udtf.py -> build\\lib\\pyspark\\sql\\worker\n",
      "  copying pyspark\\sql\\worker\\commit_data_source_write.py -> build\\lib\\pyspark\\sql\\worker\n",
      "  copying pyspark\\sql\\worker\\create_data_source.py -> build\\lib\\pyspark\\sql\\worker\n",
      "  copying pyspark\\sql\\worker\\data_source_pushdown_filters.py -> build\\lib\\pyspark\\sql\\worker\n",
      "  copying pyspark\\sql\\worker\\lookup_data_sources.py -> build\\lib\\pyspark\\sql\\worker\n",
      "  copying pyspark\\sql\\worker\\plan_data_source_read.py -> build\\lib\\pyspark\\sql\\worker\n",
      "  copying pyspark\\sql\\worker\\python_streaming_sink_runner.py -> build\\lib\\pyspark\\sql\\worker\n",
      "  copying pyspark\\sql\\worker\\write_into_data_source.py -> build\\lib\\pyspark\\sql\\worker\n",
      "  copying pyspark\\sql\\worker\\__init__.py -> build\\lib\\pyspark\\sql\\worker\n",
      "  creating build\\lib\\pyspark\\streaming\n",
      "  copying pyspark\\streaming\\context.py -> build\\lib\\pyspark\\streaming\n",
      "  copying pyspark\\streaming\\dstream.py -> build\\lib\\pyspark\\streaming\n",
      "  copying pyspark\\streaming\\kinesis.py -> build\\lib\\pyspark\\streaming\n",
      "  copying pyspark\\streaming\\listener.py -> build\\lib\\pyspark\\streaming\n",
      "  copying pyspark\\streaming\\util.py -> build\\lib\\pyspark\\streaming\n",
      "  copying pyspark\\streaming\\__init__.py -> build\\lib\\pyspark\\streaming\n",
      "  creating build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\accessors.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\base.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\categorical.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\config.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\correlation.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\datetimes.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\exceptions.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\extensions.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\frame.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\generic.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\groupby.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\indexing.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\internal.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\mlflow.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\namespace.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\numpy_compat.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\resample.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\series.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\sql_formatter.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\sql_processor.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\strings.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\supported_api_gen.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\testing.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\utils.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\window.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\_typing.py -> build\\lib\\pyspark\\pandas\n",
      "  copying pyspark\\pandas\\__init__.py -> build\\lib\\pyspark\\pandas\n",
      "  creating build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  copying pyspark\\pandas\\data_type_ops\\base.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  copying pyspark\\pandas\\data_type_ops\\binary_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  copying pyspark\\pandas\\data_type_ops\\boolean_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  copying pyspark\\pandas\\data_type_ops\\categorical_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  copying pyspark\\pandas\\data_type_ops\\complex_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  copying pyspark\\pandas\\data_type_ops\\datetime_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  copying pyspark\\pandas\\data_type_ops\\date_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  copying pyspark\\pandas\\data_type_ops\\null_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  copying pyspark\\pandas\\data_type_ops\\num_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  copying pyspark\\pandas\\data_type_ops\\string_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  copying pyspark\\pandas\\data_type_ops\\timedelta_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  copying pyspark\\pandas\\data_type_ops\\udt_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  copying pyspark\\pandas\\data_type_ops\\__init__.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "  creating build\\lib\\pyspark\\pandas\\indexes\n",
      "  copying pyspark\\pandas\\indexes\\base.py -> build\\lib\\pyspark\\pandas\\indexes\n",
      "  copying pyspark\\pandas\\indexes\\category.py -> build\\lib\\pyspark\\pandas\\indexes\n",
      "  copying pyspark\\pandas\\indexes\\datetimes.py -> build\\lib\\pyspark\\pandas\\indexes\n",
      "  copying pyspark\\pandas\\indexes\\multi.py -> build\\lib\\pyspark\\pandas\\indexes\n",
      "  copying pyspark\\pandas\\indexes\\timedelta.py -> build\\lib\\pyspark\\pandas\\indexes\n",
      "  copying pyspark\\pandas\\indexes\\__init__.py -> build\\lib\\pyspark\\pandas\\indexes\n",
      "  creating build\\lib\\pyspark\\pandas\\missing\n",
      "  copying pyspark\\pandas\\missing\\common.py -> build\\lib\\pyspark\\pandas\\missing\n",
      "  copying pyspark\\pandas\\missing\\frame.py -> build\\lib\\pyspark\\pandas\\missing\n",
      "  copying pyspark\\pandas\\missing\\general_functions.py -> build\\lib\\pyspark\\pandas\\missing\n",
      "  copying pyspark\\pandas\\missing\\groupby.py -> build\\lib\\pyspark\\pandas\\missing\n",
      "  copying pyspark\\pandas\\missing\\indexes.py -> build\\lib\\pyspark\\pandas\\missing\n",
      "  copying pyspark\\pandas\\missing\\resample.py -> build\\lib\\pyspark\\pandas\\missing\n",
      "  copying pyspark\\pandas\\missing\\scalars.py -> build\\lib\\pyspark\\pandas\\missing\n",
      "  copying pyspark\\pandas\\missing\\series.py -> build\\lib\\pyspark\\pandas\\missing\n",
      "  copying pyspark\\pandas\\missing\\window.py -> build\\lib\\pyspark\\pandas\\missing\n",
      "  copying pyspark\\pandas\\missing\\__init__.py -> build\\lib\\pyspark\\pandas\\missing\n",
      "  creating build\\lib\\pyspark\\pandas\\plot\n",
      "  copying pyspark\\pandas\\plot\\core.py -> build\\lib\\pyspark\\pandas\\plot\n",
      "  copying pyspark\\pandas\\plot\\matplotlib.py -> build\\lib\\pyspark\\pandas\\plot\n",
      "  copying pyspark\\pandas\\plot\\plotly.py -> build\\lib\\pyspark\\pandas\\plot\n",
      "  copying pyspark\\pandas\\plot\\__init__.py -> build\\lib\\pyspark\\pandas\\plot\n",
      "  creating build\\lib\\pyspark\\pandas\\spark\n",
      "  copying pyspark\\pandas\\spark\\accessors.py -> build\\lib\\pyspark\\pandas\\spark\n",
      "  copying pyspark\\pandas\\spark\\utils.py -> build\\lib\\pyspark\\pandas\\spark\n",
      "  copying pyspark\\pandas\\spark\\__init__.py -> build\\lib\\pyspark\\pandas\\spark\n",
      "  creating build\\lib\\pyspark\\pandas\\typedef\n",
      "  copying pyspark\\pandas\\typedef\\typehints.py -> build\\lib\\pyspark\\pandas\\typedef\n",
      "  copying pyspark\\pandas\\typedef\\__init__.py -> build\\lib\\pyspark\\pandas\\typedef\n",
      "  creating build\\lib\\pyspark\\pandas\\usage_logging\n",
      "  copying pyspark\\pandas\\usage_logging\\usage_logger.py -> build\\lib\\pyspark\\pandas\\usage_logging\n",
      "  copying pyspark\\pandas\\usage_logging\\__init__.py -> build\\lib\\pyspark\\pandas\\usage_logging\n",
      "  creating build\\lib\\pyspark\\pipelines\n",
      "  copying pyspark\\pipelines\\add_pipeline_analysis_context.py -> build\\lib\\pyspark\\pipelines\n",
      "  copying pyspark\\pipelines\\api.py -> build\\lib\\pyspark\\pipelines\n",
      "  copying pyspark\\pipelines\\block_connect_access.py -> build\\lib\\pyspark\\pipelines\n",
      "  copying pyspark\\pipelines\\block_session_mutations.py -> build\\lib\\pyspark\\pipelines\n",
      "  copying pyspark\\pipelines\\cli.py -> build\\lib\\pyspark\\pipelines\n",
      "  copying pyspark\\pipelines\\flow.py -> build\\lib\\pyspark\\pipelines\n",
      "  copying pyspark\\pipelines\\graph_element_registry.py -> build\\lib\\pyspark\\pipelines\n",
      "  copying pyspark\\pipelines\\init_cli.py -> build\\lib\\pyspark\\pipelines\n",
      "  copying pyspark\\pipelines\\logging_utils.py -> build\\lib\\pyspark\\pipelines\n",
      "  copying pyspark\\pipelines\\output.py -> build\\lib\\pyspark\\pipelines\n",
      "  copying pyspark\\pipelines\\source_code_location.py -> build\\lib\\pyspark\\pipelines\n",
      "  copying pyspark\\pipelines\\spark_connect_graph_element_registry.py -> build\\lib\\pyspark\\pipelines\n",
      "  copying pyspark\\pipelines\\spark_connect_pipeline.py -> build\\lib\\pyspark\\pipelines\n",
      "  copying pyspark\\pipelines\\type_error_utils.py -> build\\lib\\pyspark\\pipelines\n",
      "  copying pyspark\\pipelines\\__init__.py -> build\\lib\\pyspark\\pipelines\n",
      "  creating build\\lib\\pyspark\\python\\pyspark\n",
      "  copying pyspark\\python\\pyspark\\shell.py -> build\\lib\\pyspark\\python\\pyspark\n",
      "  creating build\\lib\\pyspark\\testing\n",
      "  copying pyspark\\testing\\connectutils.py -> build\\lib\\pyspark\\testing\n",
      "  copying pyspark\\testing\\mllibutils.py -> build\\lib\\pyspark\\testing\n",
      "  copying pyspark\\testing\\mlutils.py -> build\\lib\\pyspark\\testing\n",
      "  copying pyspark\\testing\\objects.py -> build\\lib\\pyspark\\testing\n",
      "  copying pyspark\\testing\\pandasutils.py -> build\\lib\\pyspark\\testing\n",
      "  copying pyspark\\testing\\sqlutils.py -> build\\lib\\pyspark\\testing\n",
      "  copying pyspark\\testing\\streamingutils.py -> build\\lib\\pyspark\\testing\n",
      "  copying pyspark\\testing\\utils.py -> build\\lib\\pyspark\\testing\n",
      "  copying pyspark\\testing\\__init__.py -> build\\lib\\pyspark\\testing\n",
      "  creating build\\lib\\pyspark\\resource\n",
      "  copying pyspark\\resource\\information.py -> build\\lib\\pyspark\\resource\n",
      "  copying pyspark\\resource\\profile.py -> build\\lib\\pyspark\\resource\n",
      "  copying pyspark\\resource\\requests.py -> build\\lib\\pyspark\\resource\n",
      "  copying pyspark\\resource\\__init__.py -> build\\lib\\pyspark\\resource\n",
      "  creating build\\lib\\pyspark\\errors\n",
      "  copying pyspark\\errors\\error_classes.py -> build\\lib\\pyspark\\errors\n",
      "  copying pyspark\\errors\\utils.py -> build\\lib\\pyspark\\errors\n",
      "  copying pyspark\\errors\\__init__.py -> build\\lib\\pyspark\\errors\n",
      "  creating build\\lib\\pyspark\\errors\\exceptions\n",
      "  copying pyspark\\errors\\exceptions\\base.py -> build\\lib\\pyspark\\errors\\exceptions\n",
      "  copying pyspark\\errors\\exceptions\\captured.py -> build\\lib\\pyspark\\errors\\exceptions\n",
      "  copying pyspark\\errors\\exceptions\\connect.py -> build\\lib\\pyspark\\errors\\exceptions\n",
      "  copying pyspark\\errors\\exceptions\\tblib.py -> build\\lib\\pyspark\\errors\\exceptions\n",
      "  copying pyspark\\errors\\exceptions\\__init__.py -> build\\lib\\pyspark\\errors\\exceptions\n",
      "  creating build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "  copying deps\\examples\\als.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "  copying deps\\examples\\avro_inputformat.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "  copying deps\\examples\\kmeans.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "  copying deps\\examples\\logistic_regression.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "  copying deps\\examples\\pagerank.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "  copying deps\\examples\\parquet_inputformat.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "  copying deps\\examples\\pi.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "  copying deps\\examples\\sort.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "  copying deps\\examples\\status_api_demo.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "  copying deps\\examples\\transitive_closure.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "  copying deps\\examples\\wordcount.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "  copying deps\\examples\\__init__.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "  creating build\\lib\\pyspark\\logger\n",
      "  copying pyspark\\logger\\logger.py -> build\\lib\\pyspark\\logger\n",
      "  copying pyspark\\logger\\worker_io.py -> build\\lib\\pyspark\\logger\n",
      "  copying pyspark\\logger\\__init__.py -> build\\lib\\pyspark\\logger\n",
      "  running egg_info\n",
      "  writing pyspark.egg-info\\PKG-INFO\n",
      "  writing dependency_links to pyspark.egg-info\\dependency_links.txt\n",
      "  writing requirements to pyspark.egg-info\\requires.txt\n",
      "  writing top-level names to pyspark.egg-info\\top_level.txt\n",
      "  reading manifest file 'pyspark.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no previously-included files matching '*.py[cod]' found anywhere in distribution\n",
      "  warning: no previously-included files matching '__pycache__' found anywhere in distribution\n",
      "  warning: no previously-included files matching '.DS_Store' found anywhere in distribution\n",
      "  writing manifest file 'pyspark.egg-info\\SOURCES.txt'\n",
      "  C:\\Users\\juanh\\AppData\\Local\\Temp\\pip-build-env-zbpcbxp0\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:215: _Warning: Package 'pyspark.data.graphx' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'pyspark.data.graphx' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'pyspark.data.graphx' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'pyspark.data.graphx' to be distributed and are\n",
      "          already explicitly excluding 'pyspark.data.graphx' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  C:\\Users\\juanh\\AppData\\Local\\Temp\\pip-build-env-zbpcbxp0\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:215: _Warning: Package 'pyspark.data.mllib' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'pyspark.data.mllib' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'pyspark.data.mllib' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'pyspark.data.mllib' to be distributed and are\n",
      "          already explicitly excluding 'pyspark.data.mllib' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  C:\\Users\\juanh\\AppData\\Local\\Temp\\pip-build-env-zbpcbxp0\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:215: _Warning: Package 'pyspark.data.mllib.als' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'pyspark.data.mllib.als' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'pyspark.data.mllib.als' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'pyspark.data.mllib.als' to be distributed and are\n",
      "          already explicitly excluding 'pyspark.data.mllib.als' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  C:\\Users\\juanh\\AppData\\Local\\Temp\\pip-build-env-zbpcbxp0\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:215: _Warning: Package 'pyspark.data.mllib.images' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'pyspark.data.mllib.images' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'pyspark.data.mllib.images' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'pyspark.data.mllib.images' to be distributed and are\n",
      "          already explicitly excluding 'pyspark.data.mllib.images' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  C:\\Users\\juanh\\AppData\\Local\\Temp\\pip-build-env-zbpcbxp0\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:215: _Warning: Package 'pyspark.data.mllib.images.origin' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'pyspark.data.mllib.images.origin' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'pyspark.data.mllib.images.origin' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'pyspark.data.mllib.images.origin' to be distributed and are\n",
      "          already explicitly excluding 'pyspark.data.mllib.images.origin' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  C:\\Users\\juanh\\AppData\\Local\\Temp\\pip-build-env-zbpcbxp0\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:215: _Warning: Package 'pyspark.data.mllib.images.origin.kittens' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'pyspark.data.mllib.images.origin.kittens' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'pyspark.data.mllib.images.origin.kittens' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'pyspark.data.mllib.images.origin.kittens' to be distributed and are\n",
      "          already explicitly excluding 'pyspark.data.mllib.images.origin.kittens' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  C:\\Users\\juanh\\AppData\\Local\\Temp\\pip-build-env-zbpcbxp0\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:215: _Warning: Package 'pyspark.data.streaming' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'pyspark.data.streaming' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'pyspark.data.streaming' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'pyspark.data.streaming' to be distributed and are\n",
      "          already explicitly excluding 'pyspark.data.streaming' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  C:\\Users\\juanh\\AppData\\Local\\Temp\\pip-build-env-zbpcbxp0\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:215: _Warning: Package 'pyspark.examples.src.main.python.ml' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'pyspark.examples.src.main.python.ml' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'pyspark.examples.src.main.python.ml' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'pyspark.examples.src.main.python.ml' to be distributed and are\n",
      "          already explicitly excluding 'pyspark.examples.src.main.python.ml' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  C:\\Users\\juanh\\AppData\\Local\\Temp\\pip-build-env-zbpcbxp0\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:215: _Warning: Package 'pyspark.examples.src.main.python.mllib' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'pyspark.examples.src.main.python.mllib' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'pyspark.examples.src.main.python.mllib' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'pyspark.examples.src.main.python.mllib' to be distributed and are\n",
      "          already explicitly excluding 'pyspark.examples.src.main.python.mllib' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  C:\\Users\\juanh\\AppData\\Local\\Temp\\pip-build-env-zbpcbxp0\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:215: _Warning: Package 'pyspark.examples.src.main.python.sql' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'pyspark.examples.src.main.python.sql' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'pyspark.examples.src.main.python.sql' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'pyspark.examples.src.main.python.sql' to be distributed and are\n",
      "          already explicitly excluding 'pyspark.examples.src.main.python.sql' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  C:\\Users\\juanh\\AppData\\Local\\Temp\\pip-build-env-zbpcbxp0\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:215: _Warning: Package 'pyspark.examples.src.main.python.sql.streaming' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'pyspark.examples.src.main.python.sql.streaming' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'pyspark.examples.src.main.python.sql.streaming' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'pyspark.examples.src.main.python.sql.streaming' to be distributed and are\n",
      "          already explicitly excluding 'pyspark.examples.src.main.python.sql.streaming' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  C:\\Users\\juanh\\AppData\\Local\\Temp\\pip-build-env-zbpcbxp0\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:215: _Warning: Package 'pyspark.examples.src.main.python.streaming' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'pyspark.examples.src.main.python.streaming' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'pyspark.examples.src.main.python.streaming' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'pyspark.examples.src.main.python.streaming' to be distributed and are\n",
      "          already explicitly excluding 'pyspark.examples.src.main.python.streaming' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  C:\\Users\\juanh\\AppData\\Local\\Temp\\pip-build-env-zbpcbxp0\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:215: _Warning: Package 'pyspark.sql.pandas._typing' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'pyspark.sql.pandas._typing' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'pyspark.sql.pandas._typing' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'pyspark.sql.pandas._typing' to be distributed and are\n",
      "          already explicitly excluding 'pyspark.sql.pandas._typing' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  C:\\Users\\juanh\\AppData\\Local\\Temp\\pip-build-env-zbpcbxp0\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:215: _Warning: Package 'pyspark.sql.pandas._typing.protocols' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'pyspark.sql.pandas._typing.protocols' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'pyspark.sql.pandas._typing.protocols' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'pyspark.sql.pandas._typing.protocols' to be distributed and are\n",
      "          already explicitly excluding 'pyspark.sql.pandas._typing.protocols' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  copying pyspark\\_typing.pyi -> build\\lib\\pyspark\n",
      "  copying pyspark\\py.typed -> build\\lib\\pyspark\n",
      "  copying pyspark\\mllib\\_typing.pyi -> build\\lib\\pyspark\\mllib\n",
      "  copying pyspark\\ml\\_typing.pyi -> build\\lib\\pyspark\\ml\n",
      "  copying pyspark\\sql\\_typing.pyi -> build\\lib\\pyspark\\sql\n",
      "  copying pyspark\\sql\\connect\\proto\\base_pb2.pyi -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\catalog_pb2.pyi -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\commands_pb2.pyi -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\common_pb2.pyi -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\example_plugins_pb2.pyi -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\expressions_pb2.pyi -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\ml_common_pb2.pyi -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\ml_pb2.pyi -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\pipelines_pb2.pyi -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\relations_pb2.pyi -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\connect\\proto\\types_pb2.pyi -> build\\lib\\pyspark\\sql\\connect\\proto\n",
      "  copying pyspark\\sql\\pandas\\functions.pyi -> build\\lib\\pyspark\\sql\\pandas\n",
      "  creating build\\lib\\pyspark\\sql\\pandas\\_typing\n",
      "  copying pyspark\\sql\\pandas\\_typing\\__init__.pyi -> build\\lib\\pyspark\\sql\\pandas\\_typing\n",
      "  creating build\\lib\\pyspark\\sql\\pandas\\_typing\\protocols\n",
      "  copying pyspark\\sql\\pandas\\_typing\\protocols\\__init__.pyi -> build\\lib\\pyspark\\sql\\pandas\\_typing\\protocols\n",
      "  copying pyspark\\sql\\pandas\\_typing\\protocols\\frame.pyi -> build\\lib\\pyspark\\sql\\pandas\\_typing\\protocols\n",
      "  copying pyspark\\sql\\pandas\\_typing\\protocols\\series.pyi -> build\\lib\\pyspark\\sql\\pandas\\_typing\\protocols\n",
      "  copying pyspark\\sql\\streaming\\proto\\StateMessage_pb2.pyi -> build\\lib\\pyspark\\sql\\streaming\\proto\n",
      "  creating build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\beeline -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\beeline.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\docker-image-tool.sh -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\find-spark-home -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\find-spark-home.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\load-spark-env.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\load-spark-env.sh -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\pyspark -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\pyspark.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\pyspark2.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\run-example -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\run-example.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\spark-class -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\spark-class.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\spark-class2.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\spark-connect-shell -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\spark-pipelines -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\spark-shell -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\spark-shell.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\spark-shell2.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\spark-sql -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\spark-sql.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\spark-sql2.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\spark-submit -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\spark-submit.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\spark-submit2.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\sparkR -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\sparkR.cmd -> build\\lib\\pyspark\\bin\n",
      "  copying deps\\bin\\sparkR2.cmd -> build\\lib\\pyspark\\bin\n",
      "  creating build\\lib\\pyspark\\sbin\n",
      "  copying deps\\sbin\\spark-config.sh -> build\\lib\\pyspark\\sbin\n",
      "  copying deps\\sbin\\spark-daemon.sh -> build\\lib\\pyspark\\sbin\n",
      "  copying deps\\sbin\\start-history-server.sh -> build\\lib\\pyspark\\sbin\n",
      "  copying deps\\sbin\\stop-history-server.sh -> build\\lib\\pyspark\\sbin\n",
      "  creating build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\HikariCP-2.5.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\JLargeArrays-1.5.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\JTransforms-3.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\RoaringBitmap-1.3.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\ST4-4.0.4.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\aircompressor-2.0.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\algebra_2.13-2.8.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\antlr-runtime-3.5.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\antlr4-runtime-4.13.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\aopalliance-repackaged-3.0.6.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\arpack-3.0.4.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\arpack_combined_all-0.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\arrow-compression-18.3.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\arrow-format-18.3.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\arrow-memory-core-18.3.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\arrow-memory-netty-18.3.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\arrow-memory-netty-buffer-patch-18.3.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\arrow-vector-18.3.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\audience-annotations-0.12.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\avro-1.12.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\avro-ipc-1.12.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\avro-mapred-1.12.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\blas-3.0.4.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\breeze-macros_2.13-2.1.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\breeze_2.13-2.1.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\cats-kernel_2.13-2.8.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\chill-java-0.10.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\chill_2.13-0.10.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-cli-1.10.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-codec-1.19.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-collections4-4.5.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-compiler-3.1.9.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-compress-1.28.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-crypto-1.1.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-dbcp-1.4.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-io-2.21.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-lang-2.6.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-lang3-3.19.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-math3-3.6.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-pool-1.5.4.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\commons-text-1.14.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\compress-lzf-1.1.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\curator-client-5.9.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\curator-framework-5.9.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\curator-recipes-5.9.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\datanucleus-api-jdo-4.2.4.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\datanucleus-core-4.1.17.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\datanucleus-rdbms-4.1.19.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\datasketches-java-6.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\datasketches-memory-3.0.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\derby-10.16.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\derbyshared-10.16.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\derbytools-10.16.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\failureaccess-1.0.3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\flatbuffers-java-25.2.10.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\gson-2.11.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\guava-33.4.8-jre.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hadoop-client-api-3.4.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hadoop-client-runtime-3.4.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hive-beeline-2.3.10.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hive-cli-2.3.10.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hive-common-2.3.10.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hive-exec-2.3.10-core.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hive-jdbc-2.3.10.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hive-metastore-2.3.10.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hive-serde-2.3.10.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hive-service-rpc-4.0.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hive-shims-0.23-2.3.10.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hive-shims-2.3.10.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hive-shims-common-2.3.10.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hive-shims-scheduler-2.3.10.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hive-storage-api-2.8.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hk2-api-3.0.6.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hk2-locator-3.0.6.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\hk2-utils-3.0.6.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\httpclient-4.5.14.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\httpcore-4.4.16.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\icu4j-77.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\istack-commons-runtime-4.1.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\ivy-2.5.3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jackson-annotations-2.20.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jackson-core-2.20.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jackson-databind-2.20.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jackson-dataformat-yaml-2.20.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jackson-datatype-jsr310-2.20.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jackson-module-scala_2.13-2.20.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jakarta.activation-api-2.1.3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jakarta.annotation-api-2.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jakarta.inject-api-2.0.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jakarta.servlet-api-5.0.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jakarta.validation-api-3.0.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jakarta.ws.rs-api-3.0.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jakarta.xml.bind-api-4.0.2.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\janino-3.1.9.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\java-diff-utils-4.16.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\javassist-3.30.2-GA.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\javax.jdo-3.2.0-m3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\javax.servlet-api-4.0.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\javolution-5.5.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jaxb-core-4.0.5.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jaxb-runtime-4.0.5.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jcl-over-slf4j-2.0.17.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jdo-api-3.0.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jersey-client-3.0.18.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jersey-common-3.0.18.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jersey-container-servlet-3.0.18.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jersey-container-servlet-core-3.0.18.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jersey-hk2-3.0.18.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jersey-server-3.0.18.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jjwt-api-0.12.6.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jjwt-impl-0.12.6.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jjwt-jackson-0.12.6.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jline-2.14.6.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jline-3.29.0-jdk8.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\joda-time-2.14.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jpam-1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\json-1.8.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\json4s-ast_2.13-4.0.7.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\json4s-core_2.13-4.0.7.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\json4s-jackson-core_2.13-4.0.7.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\json4s-jackson_2.13-4.0.7.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\json4s-scalap_2.13-4.0.7.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jsr305-3.0.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jta-1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jts-core-1.20.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\jul-to-slf4j-2.0.17.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kryo-shaded-4.0.3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-client-7.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-client-api-7.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-httpclient-vertx-7.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-admissionregistration-7.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-apiextensions-7.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-apps-7.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-autoscaling-7.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-batch-7.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-certificates-7.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-common-7.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-coordination-7.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-core-7.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-discovery-7.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-events-7.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-extensions-7.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-flowcontrol-7.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-gatewayapi-7.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-metrics-7.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-networking-7.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-node-7.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-policy-7.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-rbac-7.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-resource-7.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-scheduling-7.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\kubernetes-model-storageclass-7.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\lapack-3.0.4.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\leveldbjni-all-1.8.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\libfb303-0.9.3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\libthrift-0.16.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\log4j-1.2-api-2.24.3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\log4j-api-2.24.3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\log4j-core-2.24.3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\log4j-layout-template-json-2.24.3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\log4j-slf4j2-impl-2.24.3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\lz4-java-1.8.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\metrics-core-4.2.37.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\metrics-graphite-4.2.37.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\metrics-jmx-4.2.37.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\metrics-json-4.2.37.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\metrics-jvm-4.2.37.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\minlog-1.3.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-all-4.2.7.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-buffer-4.2.7.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-codec-4.2.7.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-codec-base-4.2.7.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-codec-classes-quic-4.2.7.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-codec-compression-4.2.7.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-codec-dns-4.2.7.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-codec-http-4.2.7.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-codec-http2-4.2.7.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-codec-http3-4.2.7.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-codec-marshalling-4.2.7.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-codec-native-quic-4.2.7.Final-linux-aarch_64.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-codec-native-quic-4.2.7.Final-linux-x86_64.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-codec-native-quic-4.2.7.Final-osx-aarch_64.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-codec-native-quic-4.2.7.Final-osx-x86_64.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-codec-native-quic-4.2.7.Final-windows-x86_64.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-codec-protobuf-4.2.7.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-codec-socks-4.2.7.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-common-4.2.7.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-handler-4.2.7.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-handler-proxy-4.2.7.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-resolver-4.2.7.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-resolver-dns-4.2.7.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-tcnative-boringssl-static-2.0.74.Final-linux-aarch_64.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-tcnative-boringssl-static-2.0.74.Final-linux-x86_64.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-tcnative-boringssl-static-2.0.74.Final-osx-aarch_64.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-tcnative-boringssl-static-2.0.74.Final-osx-x86_64.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-tcnative-boringssl-static-2.0.74.Final-windows-x86_64.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-tcnative-classes-2.0.74.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-transport-4.2.7.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-transport-classes-epoll-4.2.7.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-transport-classes-io_uring-4.2.7.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-transport-classes-kqueue-4.2.7.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-transport-native-epoll-4.2.7.Final-linux-aarch_64.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-transport-native-epoll-4.2.7.Final-linux-riscv64.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-transport-native-epoll-4.2.7.Final-linux-x86_64.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-transport-native-io_uring-4.2.7.Final-linux-aarch_64.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-transport-native-io_uring-4.2.7.Final-linux-riscv64.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-transport-native-io_uring-4.2.7.Final-linux-x86_64.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-transport-native-kqueue-4.2.7.Final-osx-aarch_64.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-transport-native-kqueue-4.2.7.Final-osx-x86_64.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\netty-transport-native-unix-common-4.2.7.Final.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\objenesis-3.4.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\opencsv-2.3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\orc-core-2.2.1-shaded-protobuf.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\orc-format-1.1.1-shaded-protobuf.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\orc-mapreduce-2.2.1-shaded-protobuf.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\orc-shims-2.2.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\oro-2.0.8.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\osgi-resource-locator-1.0.3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\paranamer-2.8.3.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\parquet-column-1.16.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\parquet-common-1.16.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\parquet-encoding-1.16.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\parquet-format-structures-1.16.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\parquet-hadoop-1.16.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\parquet-jackson-1.16.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\pickle-1.5.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\py4j-0.10.9.9.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\rocksdbjni-9.8.4.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\scala-compiler-2.13.17.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\scala-library-2.13.17.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\scala-parallel-collections_2.13-1.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\scala-parser-combinators_2.13-2.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\scala-reflect-2.13.17.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\scala-xml_2.13-2.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\slf4j-api-2.0.17.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\snakeyaml-2.4.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\snakeyaml-engine-2.10.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\snappy-java-1.1.10.8.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-catalyst_2.13-4.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-common-utils-java_2.13-4.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-common-utils_2.13-4.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-connect_2.13-4.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-core_2.13-4.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-graphx_2.13-4.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-hive-thriftserver_2.13-4.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-hive_2.13-4.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-kubernetes_2.13-4.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-kvstore_2.13-4.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-launcher_2.13-4.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-mllib-local_2.13-4.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-mllib_2.13-4.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-network-common_2.13-4.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-network-shuffle_2.13-4.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-pipelines_2.13-4.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-repl_2.13-4.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-sketch_2.13-4.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-sql-api_2.13-4.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-sql_2.13-4.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-streaming_2.13-4.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-tags_2.13-4.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-unsafe_2.13-4.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-variant_2.13-4.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spark-yarn_2.13-4.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spire-macros_2.13-0.18.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spire-platform_2.13-0.18.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spire-util_2.13-0.18.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\spire_2.13-0.18.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\stax-api-1.0.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\stream-2.9.8.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\super-csv-2.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\threeten-extra-1.8.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\tink-1.16.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\transaction-api-1.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\univocity-parsers-2.9.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\vertx-auth-common-4.5.14.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\vertx-core-4.5.14.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\vertx-web-client-4.5.14.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\vertx-web-common-4.5.14.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\xbean-asm9-shaded-4.28.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\xmlschema-core-2.3.1.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\xz-1.10.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\zjsonpatch-7.4.0.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\zookeeper-3.9.4.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\zookeeper-jute-3.9.4.jar -> build\\lib\\pyspark\\jars\n",
      "  copying deps\\jars\\zstd-jni-1.5.7-6.jar -> build\\lib\\pyspark\\jars\n",
      "  creating build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\ammonite-compiler-2.13.17_2.13.17-3.0.3.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\ammonite-compiler-interface-2.13.17_2.13.17-3.0.3.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\ammonite-interp-2.13.17_2.13.17-3.0.3.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\ammonite-interp-api-2.13.17_2.13.17-3.0.3.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\ammonite-repl-2.13.17_2.13.17-3.0.3.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\ammonite-repl-api-2.13.17_2.13.17-3.0.3.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\ammonite-runtime-2.13.17_2.13.17-3.0.3.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\ammonite-terminal-2.13.17_2.13-3.0.3.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\ammonite-util-2.13.17_2.13-3.0.3.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\ammonite_2.13.17-3.0.3.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\bsp4j-2.1.1.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\class-path-util_2.13-0.1.4.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\common_2.13-4.13.10.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\dependency-interface_2.13-0.3.2.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\dependency_2.13-0.3.2.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\fansi_2.13-0.5.1.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\fastparse_2.13-3.1.1.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\geny_2.13-1.1.1.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\interface-1.0.28.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\io_2.13-4.13.10.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\javaparser-core-3.2.12.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\lenses_2.13-0.11.15.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\mainargs_2.13-0.7.6.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\org.eclipse.lsp4j.generator-0.20.1.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\org.eclipse.lsp4j.jsonrpc-0.20.1.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\org.eclipse.xtend.lib-2.28.0.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\org.eclipse.xtend.lib.macro-2.28.0.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\org.eclipse.xtext.xbase.lib-2.28.0.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\os-lib_2.13-0.11.5.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\os-zip-0.11.5.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\parsers_2.13-4.13.10.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\pprint_2.13-0.9.0.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\requests_2.13-0.9.0.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\scalameta_2.13-4.13.10.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\scalaparse_2.13-3.1.1.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\scalapb-runtime_2.13-0.11.15.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\semanticdb-shared_2.13-4.13.10.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\sourcecode_2.13-0.4.4.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\spark-connect-client-jdbc_2.13-4.1.1.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\spark-connect-client-jvm_2.13-4.1.1.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\trees_2.13-4.13.10.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\typename_2.13-1.1.0.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\ujson_2.13-4.3.2.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\upack_2.13-4.3.2.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\upickle-core_2.13-4.3.2.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\upickle-implicits_2.13-4.3.2.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  copying deps\\jars\\connect-repl\\upickle_2.13-4.3.2.jar -> build\\lib\\pyspark\\jars\\connect-repl\n",
      "  creating build\\lib\\pyspark\\python\\lib\n",
      "  copying lib\\py4j-0.10.9.9-src.zip -> build\\lib\\pyspark\\python\\lib\n",
      "  copying lib\\pyspark.zip -> build\\lib\\pyspark\\python\\lib\n",
      "  creating build\\lib\\pyspark\\data\\artifact-tests\\crc\n",
      "  copying deps\\data\\artifact-tests\\crc\\junitLargeJar.txt -> build\\lib\\pyspark\\data\\artifact-tests\\crc\n",
      "  copying deps\\data\\artifact-tests\\crc\\smallJar.txt -> build\\lib\\pyspark\\data\\artifact-tests\\crc\n",
      "  creating build\\lib\\pyspark\\data\\graphx\n",
      "  copying deps\\data\\graphx\\followers.txt -> build\\lib\\pyspark\\data\\graphx\n",
      "  copying deps\\data\\graphx\\users.txt -> build\\lib\\pyspark\\data\\graphx\n",
      "  creating build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\gmm_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\kmeans_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\pagerank_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\pic_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\sample_binary_classification_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\sample_fpgrowth.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\sample_isotonic_regression_libsvm_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\sample_kmeans_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\sample_lda_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\sample_lda_libsvm_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\sample_libsvm_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\sample_linear_regression_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\sample_movielens_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\sample_multiclass_classification_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\sample_svm_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  copying deps\\data\\mllib\\streaming_kmeans_data_test.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "  creating build\\lib\\pyspark\\data\\mllib\\als\n",
      "  copying deps\\data\\mllib\\als\\sample_movielens_ratings.txt -> build\\lib\\pyspark\\data\\mllib\\als\n",
      "  copying deps\\data\\mllib\\als\\test.data -> build\\lib\\pyspark\\data\\mllib\\als\n",
      "  creating build\\lib\\pyspark\\data\\mllib\\images\n",
      "  copying deps\\data\\mllib\\images\\license.txt -> build\\lib\\pyspark\\data\\mllib\\images\n",
      "  creating build\\lib\\pyspark\\data\\mllib\\images\\origin\n",
      "  copying deps\\data\\mllib\\images\\origin\\license.txt -> build\\lib\\pyspark\\data\\mllib\\images\\origin\n",
      "  creating build\\lib\\pyspark\\data\\mllib\\images\\origin\\kittens\n",
      "  copying deps\\data\\mllib\\images\\origin\\kittens\\not-image.txt -> build\\lib\\pyspark\\data\\mllib\\images\\origin\\kittens\n",
      "  creating build\\lib\\pyspark\\data\\mllib\\ridge-data\n",
      "  copying deps\\data\\mllib\\ridge-data\\lpsa.data -> build\\lib\\pyspark\\data\\mllib\\ridge-data\n",
      "  creating build\\lib\\pyspark\\data\\streaming\n",
      "  copying deps\\data\\streaming\\AFINN-111.txt -> build\\lib\\pyspark\\data\\streaming\n",
      "  creating build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-AnchorJS.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-CC0.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-bootstrap.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-cloudpickle.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-d3.min.js.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-dagre-d3.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-datatables.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-graphlib-dot.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-jdom.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-join.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-jquery.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-json-formatter.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-loose-version.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-matchMedia-polyfill.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-modernizr.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-mustache.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-py4j.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-respond.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-sbt-launch-lib.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-sorttable.js.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-vis-timeline.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying deps\\licenses\\LICENSE-xz.txt -> build\\lib\\pyspark\\licenses\n",
      "  copying pyspark\\errors\\error-conditions.json -> build\\lib\\pyspark\\errors\n",
      "  creating build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\aft_survival_regression.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\als_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\binarizer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\bisecting_k_means_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\bucketed_random_projection_lsh_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\bucketizer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\chi_square_test_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\chisq_selector_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\correlation_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\count_vectorizer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\cross_validator.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\dataframe_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\dct_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\decision_tree_classification_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\decision_tree_regression_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\elementwise_product_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\estimator_transformer_param_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\feature_hasher_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\fm_classifier_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\fm_regressor_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\fpgrowth_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\gaussian_mixture_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\generalized_linear_regression_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\gradient_boosted_tree_classifier_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\gradient_boosted_tree_regressor_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\imputer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\index_to_string_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\interaction_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\isotonic_regression_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\kmeans_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\lda_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\linear_regression_with_elastic_net.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\linearsvc.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\logistic_regression_summary_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\logistic_regression_with_elastic_net.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\max_abs_scaler_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\min_hash_lsh_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\min_max_scaler_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\multiclass_logistic_regression_with_elastic_net.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\multilayer_perceptron_classification.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\n_gram_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\naive_bayes_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\normalizer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\one_vs_rest_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\onehot_encoder_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\pca_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\pipeline_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\polynomial_expansion_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\power_iteration_clustering_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\prefixspan_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\quantile_discretizer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\random_forest_classifier_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\random_forest_regressor_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\rformula_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\robust_scaler_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\sql_transformer.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\standard_scaler_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\stopwords_remover_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\string_indexer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\summarizer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\target_encoder_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\tf_idf_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\tokenizer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\train_validation_split.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\univariate_feature_selector_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\variance_threshold_selector_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\vector_assembler_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\vector_indexer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\vector_size_hint_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\vector_slicer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying deps\\examples\\ml\\word2vec_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  creating build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\__init__.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\binary_classification_metrics_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\bisecting_k_means_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\correlations.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\correlations_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\decision_tree_classification_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\decision_tree_regression_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\elementwise_product_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\fpgrowth_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\gaussian_mixture_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\gaussian_mixture_model.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\gradient_boosting_classification_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\gradient_boosting_regression_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\hypothesis_testing_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\hypothesis_testing_kolmogorov_smirnov_test_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\isotonic_regression_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\k_means_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\kernel_density_estimation_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\kmeans.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\latent_dirichlet_allocation_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\linear_regression_with_sgd_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\logistic_regression.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\logistic_regression_with_lbfgs_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\multi_class_metrics_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\multi_label_metrics_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\naive_bayes_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\normalizer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\pca_rowmatrix_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\power_iteration_clustering_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\random_forest_classification_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\random_forest_regression_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\random_rdd_generation.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\ranking_metrics_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\recommendation_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\regression_metrics_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\sampled_rdds.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\standard_scaler_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\stratified_sampling_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\streaming_k_means_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\streaming_linear_regression_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\summary_statistics_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\svd_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\svm_with_sgd_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\tf_idf_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\word2vec.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying deps\\examples\\mllib\\word2vec_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  creating build\\lib\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  copying deps\\examples\\sql\\__init__.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  copying deps\\examples\\sql\\arrow.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  copying deps\\examples\\sql\\basic.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  copying deps\\examples\\sql\\datasource.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  copying deps\\examples\\sql\\hive.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  copying deps\\examples\\sql\\udtf.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  creating build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "  copying deps\\examples\\sql\\streaming\\structured_kafka_wordcount.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "  copying deps\\examples\\sql\\streaming\\structured_network_wordcount.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "  copying deps\\examples\\sql\\streaming\\structured_network_wordcount_session_window.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "  copying deps\\examples\\sql\\streaming\\structured_network_wordcount_windowed.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "  copying deps\\examples\\sql\\streaming\\structured_sessionization.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "  creating build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying deps\\examples\\streaming\\__init__.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying deps\\examples\\streaming\\hdfs_wordcount.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying deps\\examples\\streaming\\network_wordcount.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying deps\\examples\\streaming\\network_wordjoinsentiments.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying deps\\examples\\streaming\\queue_stream.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying deps\\examples\\streaming\\recoverable_network_wordcount.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying deps\\examples\\streaming\\sql_network_wordcount.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying deps\\examples\\streaming\\stateful_network_wordcount.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  running build_scripts\n",
      "  creating build\\scripts-3.13\n",
      "  copying deps\\bin\\beeline -> build\\scripts-3.13\n",
      "  copying deps\\bin\\beeline.cmd -> build\\scripts-3.13\n",
      "  copying deps\\bin\\docker-image-tool.sh -> build\\scripts-3.13\n",
      "  copying deps\\bin\\find-spark-home -> build\\scripts-3.13\n",
      "  copying deps\\bin\\find-spark-home.cmd -> build\\scripts-3.13\n",
      "  copying deps\\bin\\load-spark-env.cmd -> build\\scripts-3.13\n",
      "  copying deps\\bin\\load-spark-env.sh -> build\\scripts-3.13\n",
      "  copying deps\\bin\\pyspark -> build\\scripts-3.13\n",
      "  copying deps\\bin\\pyspark.cmd -> build\\scripts-3.13\n",
      "  copying deps\\bin\\pyspark2.cmd -> build\\scripts-3.13\n",
      "  copying deps\\bin\\run-example -> build\\scripts-3.13\n",
      "  copying deps\\bin\\run-example.cmd -> build\\scripts-3.13\n",
      "  copying deps\\bin\\spark-class -> build\\scripts-3.13\n",
      "  copying deps\\bin\\spark-class.cmd -> build\\scripts-3.13\n",
      "  copying deps\\bin\\spark-class2.cmd -> build\\scripts-3.13\n",
      "  copying deps\\bin\\spark-connect-shell -> build\\scripts-3.13\n",
      "  copying deps\\bin\\spark-pipelines -> build\\scripts-3.13\n",
      "  copying deps\\bin\\spark-shell -> build\\scripts-3.13\n",
      "  copying deps\\bin\\spark-shell.cmd -> build\\scripts-3.13\n",
      "  copying deps\\bin\\spark-shell2.cmd -> build\\scripts-3.13\n",
      "  copying deps\\bin\\spark-sql -> build\\scripts-3.13\n",
      "  copying deps\\bin\\spark-sql.cmd -> build\\scripts-3.13\n",
      "  copying deps\\bin\\spark-sql2.cmd -> build\\scripts-3.13\n",
      "  copying deps\\bin\\spark-submit -> build\\scripts-3.13\n",
      "  copying deps\\bin\\spark-submit.cmd -> build\\scripts-3.13\n",
      "  copying deps\\bin\\spark-submit2.cmd -> build\\scripts-3.13\n",
      "  copying deps\\bin\\sparkR -> build\\scripts-3.13\n",
      "  copying deps\\bin\\sparkR.cmd -> build\\scripts-3.13\n",
      "  copying deps\\bin\\sparkR2.cmd -> build\\scripts-3.13\n",
      "  copying and adjusting pyspark\\find_spark_home.py -> build\\scripts-3.13\n",
      "  installing to build\\bdist.win-amd64\\wheel\n",
      "  running install\n",
      "  running install_lib\n",
      "  creating build\\bdist.win-amd64\\wheel\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\n",
      "  copying build\\lib\\pyspark\\accumulators.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\beeline -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\beeline.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\docker-image-tool.sh -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\find-spark-home -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\find-spark-home.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\load-spark-env.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\load-spark-env.sh -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\pyspark -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\pyspark.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\pyspark2.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\run-example -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\run-example.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\spark-class -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\spark-class.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\spark-class2.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\spark-connect-shell -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\spark-pipelines -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\spark-shell -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\spark-shell.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\spark-shell2.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\spark-sql -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\spark-sql.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\spark-sql2.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\spark-submit -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\spark-submit.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\spark-submit2.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\sparkR -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\sparkR.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  copying build\\lib\\pyspark\\bin\\sparkR2.cmd -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\bin\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\cloudpickle\n",
      "  copying build\\lib\\pyspark\\cloudpickle\\cloudpickle.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\cloudpickle\n",
      "  copying build\\lib\\pyspark\\cloudpickle\\cloudpickle_fast.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\cloudpickle\n",
      "  copying build\\lib\\pyspark\\cloudpickle\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\cloudpickle\n",
      "  copying build\\lib\\pyspark\\conf.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\core\n",
      "  copying build\\lib\\pyspark\\core\\broadcast.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\core\n",
      "  copying build\\lib\\pyspark\\core\\context.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\core\n",
      "  copying build\\lib\\pyspark\\core\\files.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\core\n",
      "  copying build\\lib\\pyspark\\core\\rdd.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\core\n",
      "  copying build\\lib\\pyspark\\core\\status.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\core\n",
      "  copying build\\lib\\pyspark\\core\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\core\n",
      "  copying build\\lib\\pyspark\\daemon.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\data\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\data\\artifact-tests\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\data\\artifact-tests\\crc\n",
      "  copying build\\lib\\pyspark\\data\\artifact-tests\\crc\\junitLargeJar.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\artifact-tests\\crc\n",
      "  copying build\\lib\\pyspark\\data\\artifact-tests\\crc\\smallJar.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\artifact-tests\\crc\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\data\\graphx\n",
      "  copying build\\lib\\pyspark\\data\\graphx\\followers.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\graphx\n",
      "  copying build\\lib\\pyspark\\data\\graphx\\users.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\graphx\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\data\\mllib\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\data\\mllib\\als\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\als\\sample_movielens_ratings.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\\als\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\als\\test.data -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\\als\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\gmm_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\data\\mllib\\images\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\images\\license.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\\images\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\data\\mllib\\images\\origin\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\data\\mllib\\images\\origin\\kittens\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\images\\origin\\kittens\\not-image.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\\images\\origin\\kittens\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\images\\origin\\license.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\\images\\origin\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\kmeans_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\pagerank_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\pic_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\data\\mllib\\ridge-data\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\ridge-data\\lpsa.data -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\\ridge-data\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\sample_binary_classification_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\sample_fpgrowth.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\sample_isotonic_regression_libsvm_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\sample_kmeans_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\sample_lda_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\sample_lda_libsvm_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\sample_libsvm_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\sample_linear_regression_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\sample_movielens_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\sample_multiclass_classification_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\sample_svm_data.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  copying build\\lib\\pyspark\\data\\mllib\\streaming_kmeans_data_test.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\mllib\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\data\\streaming\n",
      "  copying build\\lib\\pyspark\\data\\streaming\\AFINN-111.txt -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\data\\streaming\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\errors\n",
      "  copying build\\lib\\pyspark\\errors\\error-conditions.json -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\errors\n",
      "  copying build\\lib\\pyspark\\errors\\error_classes.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\errors\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\errors\\exceptions\n",
      "  copying build\\lib\\pyspark\\errors\\exceptions\\base.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\errors\\exceptions\n",
      "  copying build\\lib\\pyspark\\errors\\exceptions\\captured.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\errors\\exceptions\n",
      "  copying build\\lib\\pyspark\\errors\\exceptions\\connect.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\errors\\exceptions\n",
      "  copying build\\lib\\pyspark\\errors\\exceptions\\tblib.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\errors\\exceptions\n",
      "  copying build\\lib\\pyspark\\errors\\exceptions\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\errors\\exceptions\n",
      "  copying build\\lib\\pyspark\\errors\\utils.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\errors\n",
      "  copying build\\lib\\pyspark\\errors\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\errors\n",
      "  copying build\\lib\\pyspark\\errors_doc_gen.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\examples\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\examples\\src\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\examples\\src\\main\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\examples\\src\\main\\python\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\als.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\avro_inputformat.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\kmeans.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\logistic_regression.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\aft_survival_regression.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\als_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\binarizer_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\bisecting_k_means_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\bucketed_random_projection_lsh_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\bucketizer_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\chisq_selector_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\chi_square_test_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\correlation_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\count_vectorizer_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\cross_validator.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\dataframe_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\dct_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\decision_tree_classification_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\decision_tree_regression_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\elementwise_product_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\estimator_transformer_param_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\feature_hasher_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\fm_classifier_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\fm_regressor_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\fpgrowth_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\gaussian_mixture_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\generalized_linear_regression_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\gradient_boosted_tree_classifier_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\gradient_boosted_tree_regressor_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\imputer_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\index_to_string_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\interaction_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\isotonic_regression_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\kmeans_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\lda_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\linearsvc.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\linear_regression_with_elastic_net.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\logistic_regression_summary_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\logistic_regression_with_elastic_net.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\max_abs_scaler_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\min_hash_lsh_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\min_max_scaler_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\multiclass_logistic_regression_with_elastic_net.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\multilayer_perceptron_classification.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\naive_bayes_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\normalizer_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\n_gram_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\onehot_encoder_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\one_vs_rest_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\pca_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\pipeline_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\polynomial_expansion_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\power_iteration_clustering_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\prefixspan_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\quantile_discretizer_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\random_forest_classifier_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\random_forest_regressor_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\rformula_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\robust_scaler_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\sql_transformer.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\standard_scaler_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\stopwords_remover_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\string_indexer_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\summarizer_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\target_encoder_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\tf_idf_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\tokenizer_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\train_validation_split.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\univariate_feature_selector_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\variance_threshold_selector_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\vector_assembler_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\vector_indexer_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\vector_size_hint_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\vector_slicer_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\ml\\word2vec_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\ml\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\binary_classification_metrics_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\bisecting_k_means_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\correlations.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\correlations_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\decision_tree_classification_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\decision_tree_regression_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\elementwise_product_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\fpgrowth_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\gaussian_mixture_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\gaussian_mixture_model.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\gradient_boosting_classification_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\gradient_boosting_regression_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\hypothesis_testing_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\hypothesis_testing_kolmogorov_smirnov_test_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\isotonic_regression_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\kernel_density_estimation_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\kmeans.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\k_means_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\latent_dirichlet_allocation_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\linear_regression_with_sgd_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\logistic_regression.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\logistic_regression_with_lbfgs_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\multi_class_metrics_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\multi_label_metrics_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\naive_bayes_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\normalizer_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\pca_rowmatrix_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\power_iteration_clustering_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\random_forest_classification_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\random_forest_regression_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\random_rdd_generation.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\ranking_metrics_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\recommendation_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\regression_metrics_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\sampled_rdds.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\standard_scaler_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\stratified_sampling_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\streaming_k_means_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\streaming_linear_regression_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\summary_statistics_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\svd_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\svm_with_sgd_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\tf_idf_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\word2vec.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\word2vec_example.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\pagerank.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\parquet_inputformat.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\pi.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\sort.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\arrow.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\basic.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\datasource.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\hive.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\streaming\\structured_kafka_wordcount.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\streaming\\structured_network_wordcount.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\streaming\\structured_network_wordcount_session_window.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\streaming\\structured_network_wordcount_windowed.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\streaming\\structured_sessionization.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\udtf.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\sql\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\status_api_demo.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\\hdfs_wordcount.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\\network_wordcount.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\\network_wordjoinsentiments.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\\queue_stream.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\\recoverable_network_wordcount.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\\sql_network_wordcount.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\\stateful_network_wordcount.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\transitive_closure.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\wordcount.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\n",
      "  copying build\\lib\\pyspark\\examples\\src\\main\\python\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\examples\\src\\main\\python\n",
      "  copying build\\lib\\pyspark\\find_spark_home.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  copying build\\lib\\pyspark\\install.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  copying build\\lib\\pyspark\\instrumentation_utils.py -> build\\bdist.win-amd64\\wheel\\.\\pyspark\n",
      "  creating build\\bdist.win-amd64\\wheel\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\aircompressor-2.0.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\algebra_2.13-2.8.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\antlr-runtime-3.5.2.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\antlr4-runtime-4.13.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\aopalliance-repackaged-3.0.6.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\arpack-3.0.4.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\arpack_combined_all-0.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\arrow-compression-18.3.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\arrow-format-18.3.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\arrow-memory-core-18.3.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\arrow-memory-netty-18.3.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\arrow-memory-netty-buffer-patch-18.3.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\arrow-vector-18.3.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\audience-annotations-0.12.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\avro-1.12.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\avro-ipc-1.12.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\avro-mapred-1.12.1.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\blas-3.0.4.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\breeze-macros_2.13-2.1.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\breeze_2.13-2.1.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  copying build\\lib\\pyspark\\jars\\cats-kernel_2.13-2.8.0.jar -> build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\n",
      "  error: could not write to 'build\\bdist.win-amd64\\wheel\\.\\pyspark\\jars\\cats-kernel_2.13-2.8.0.jar': No space left on device\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for pyspark\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Failed to build installable wheels for some pyproject.toml based projects (pyspark)\n"
     ]
    }
   ],
   "source": [
    "!pip install cassandra-driver clickhouse-driver pandas pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9956d3bb-345d-4efb-be11-353cb1bc1b1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cassandra'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcassandra\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Cluster\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Conexin a Cassandra (Docker)\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cassandra'"
     ]
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "import random\n",
    "\n",
    "# Conexin a Cassandra (Docker)\n",
    "cluster = Cluster(['127.0.0.1'])\n",
    "session = cluster.connect()\n",
    "\n",
    "# Crear espacio de trabajo y tabla\n",
    "session.execute(\"CREATE KEYSPACE IF NOT EXISTS proyecto_bigdata WITH replication = {'class':'SimpleStrategy', 'replication_factor':1};\")\n",
    "session.set_keyspace('proyecto_bigdata')\n",
    "session.execute(\"CREATE TABLE IF NOT EXISTS ventas_crudas (id int PRIMARY KEY, categoria text, precio float, fecha timestamp);\")\n",
    "\n",
    "# Insertar 100,000 registros\n",
    "print(\"Cargando datos en Cassandra...\")\n",
    "for i in range(100000):\n",
    "    cat = random.choice(['Electronica', 'Ropa', 'Hogar', 'Juguetes'])\n",
    "    precio = random.uniform(5.0, 1000.0)\n",
    "    session.execute(f\"INSERT INTO ventas_crudas (id, categoria, precio, fecha) VALUES ({i}, '{cat}', {precio}, toTimestamp(now()))\")\n",
    "\n",
    "print(\" 100,000 registros cargados en la tabla 'ventas_crudas'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92091e05-8706-4c7e-93c1-0721c9b130cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functions \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[32m      4\u001b[39m spark = SparkSession.builder \\\n\u001b[32m      5\u001b[39m     .appName(\u001b[33m\"\u001b[39m\u001b[33mETL_UNEG\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      6\u001b[39m     .config(\u001b[33m\"\u001b[39m\u001b[33mspark.cassandra.connection.host\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m127.0.0.1\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      7\u001b[39m     .getOrCreate()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL_UNEG\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Leer de Cassandra\n",
    "df = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"ventas_crudas\", keyspace=\"proyecto_bigdata\") \\\n",
    "    .load()\n",
    "\n",
    "# Transformacin: Agrupar por categora y sumar\n",
    "ventas_resumen = df.groupBy(\"categoria\").agg(\n",
    "    F.sum(\"precio\").alias(\"total_ventas\"),\n",
    "    F.count(\"id\").alias(\"cantidad_transacciones\")\n",
    ")\n",
    "\n",
    "ventas_resumen.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6582d59c-a5ca-4d81-b6c2-b4125c9c5d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clickhouse_driver import Client\n",
    "\n",
    "client = Client(host='localhost')\n",
    "client.execute('CREATE TABLE IF NOT EXISTS ventas_resumen (categoria String, total_ventas Float64, cantidad_transacciones UInt64) ENGINE = Memory')\n",
    "\n",
    "# Convertir Spark DF a Pandas para subirlo a ClickHouse fcilmente\n",
    "pandas_df = ventas_resumen.toPandas()\n",
    "client.execute('INSERT INTO ventas_resumen VALUES', pandas_df.to_dict('records'))\n",
    "\n",
    "print(\" Datos cargados con xito en la tabla 'ventas_resumen' de ClickHouse.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239188c6-b990-47e9-8c4c-6874cdc30104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
