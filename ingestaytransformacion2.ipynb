{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "136615e0-7f6b-435f-99f4-15752d634626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sesi√≥n de Spark inicializada con el conector de Cassandra.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, count, round\n",
    "\n",
    "# Detener sesiones previas para evitar conflictos de configuraci√≥n\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Inicializar sesi√≥n de Spark con el conector de Cassandra\n",
    "# Usamos el paquete oficial de Datastax para asegurar compatibilidad\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Pipeline_BigData_Fase3\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.0\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"cassandra_db\") \\\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\") \\\n",
    "    .config(\"spark.sql.extensions\", \"com.datastax.spark.connector.CassandraSparkExtensions\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Sesi√≥n de Spark inicializada con el conector de Cassandra.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81e43af2-dd80-493c-8dad-640241b0f7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Resumen anal√≠tico generado (Vista previa):\n",
      "+-------------------+-----------+--------------+----------------------+\n",
      "|        fecha_venta|  categoria|ventas_totales|cantidad_transacciones|\n",
      "+-------------------+-----------+--------------+----------------------+\n",
      "|2025-12-16 00:00:00|  Alimentos|     329990.06|                   656|\n",
      "|2025-12-28 00:00:00|  Alimentos|     336338.33|                   642|\n",
      "|2026-02-03 00:00:00|       Ropa|     334491.98|                   678|\n",
      "|2026-01-21 00:00:00|  Alimentos|     331743.40|                   666|\n",
      "|2026-01-09 00:00:00|   Deportes|     342667.70|                   690|\n",
      "|2025-12-28 00:00:00|   Deportes|     325962.41|                   640|\n",
      "|2026-01-05 00:00:00|  Alimentos|     320445.81|                   642|\n",
      "|2026-01-25 00:00:00|Electr√≥nica|     339704.86|                   689|\n",
      "|2026-02-03 00:00:00|Electr√≥nica|     369185.57|                   688|\n",
      "|2025-12-22 00:00:00|      Hogar|     320625.39|                   631|\n",
      "+-------------------+-----------+--------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- fecha_venta: timestamp (nullable = true)\n",
      " |-- categoria: string (nullable = true)\n",
      " |-- ventas_totales: decimal(23,2) (nullable = true)\n",
      " |-- cantidad_transacciones: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fase 3.1: Lectura distribuida desde el Keyspace de Cassandra\n",
    "df_crudo = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"ventas_crudas\", keyspace=\"proyecto_bigdata\") \\\n",
    "    .load()\n",
    "\n",
    "# Fase 3.3: Limpieza de datos (eliminaci√≥n de montos inv√°lidos)\n",
    "df_limpio = df_crudo.filter(col(\"monto_total\") > 0)\n",
    "\n",
    "# Fase 3.2: L√≥gica de Agregaci√≥n\n",
    "# Consolidamos 100k registros en un resumen diario por categor√≠a\n",
    "df_resumen = df_limpio.groupBy(\"fecha_venta\", \"categoria\") \\\n",
    "    .agg(\n",
    "        round(sum(\"monto_total\"), 2).alias(\"ventas_totales\"),\n",
    "        count(\"id_venta\").alias(\"cantidad_transacciones\")\n",
    "    )\n",
    "\n",
    "print(\"üìä Resumen anal√≠tico generado (Vista previa):\")\n",
    "df_resumen.show(10)\n",
    "df_resumen.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40aab025-b5ea-4c6e-b425-f93a7384eba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: clickhouse-connect in /opt/conda/lib/python3.11/site-packages (0.10.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from clickhouse-connect) (2023.7.22)\n",
      "Requirement already satisfied: urllib3>=1.26 in /opt/conda/lib/python3.11/site-packages (from clickhouse-connect) (2.0.7)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.11/site-packages (from clickhouse-connect) (2023.3.post1)\n",
      "Requirement already satisfied: zstandard in /opt/conda/lib/python3.11/site-packages (from clickhouse-connect) (0.21.0)\n",
      "Requirement already satisfied: lz4 in /opt/conda/lib/python3.11/site-packages (from clickhouse-connect) (4.3.2)\n"
     ]
    }
   ],
   "source": [
    "# Instalaci√≥n de cliente ligero para ClickHouse (v√≠a protocolo HTTP)\n",
    "!pip install clickhouse-connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "695ca0b4-9e76-4399-baba-b8e9a4b87fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Extrayendo registros de la capa de procesamiento...\n",
      "‚úÖ CARGA EXITOSA: 300 filas migradas a dw_analitico.ventas_resumen.\n",
      "üöÄ Pipeline completo: Cassandra -> Spark -> ClickHouse operativo.\n"
     ]
    }
   ],
   "source": [
    "import clickhouse_connect\n",
    "\n",
    "print(\"üì¶ Extrayendo registros de la capa de procesamiento...\")\n",
    "# Convertimos el DataFrame a una lista de Python para la inserci√≥n masiva\n",
    "registros = [list(row) for row in df_resumen.collect()]\n",
    "\n",
    "try:\n",
    "    # Conexi√≥n directa al Data Warehouse ClickHouse\n",
    "    client = clickhouse_connect.get_client(\n",
    "        host='172.18.0.3', \n",
    "        port=8123, \n",
    "        username='default', \n",
    "        password=''\n",
    "    )\n",
    "\n",
    "    # Creamos el esquema dw_analitico seg√∫n especificaciones de la Tarea 4.1\n",
    "    client.command(\"CREATE DATABASE IF NOT EXISTS dw_analitico\")\n",
    "    \n",
    "    # Creamos la tabla con motor MergeTree para optimizar consultas de agregaci√≥n\n",
    "    client.command(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS dw_analitico.ventas_resumen (\n",
    "            fecha_venta Date,\n",
    "            categoria String,\n",
    "            ventas_totales Float64,\n",
    "            cantidad_transacciones Int64\n",
    "        ) ENGINE = MergeTree() ORDER BY (fecha_venta, categoria)\n",
    "    \"\"\")\n",
    "\n",
    "    # Inserci√≥n de los datos transformados\n",
    "    client.insert('dw_analitico.ventas_resumen', registros, \n",
    "                  column_names=['fecha_venta', 'categoria', 'ventas_totales', 'cantidad_transacciones'])\n",
    "\n",
    "    print(f\"‚úÖ CARGA EXITOSA: {len(registros)} filas migradas a dw_analitico.ventas_resumen.\")\n",
    "    print(\"üöÄ Pipeline completo: Cassandra -> Spark -> ClickHouse operativo.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error durante la carga: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8357e5a3-9125-43ec-83c1-ea0a247df8de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
