# ðŸš€ Pipeline de Big Data: Del Origen al AnÃ¡lisis (UNEG)

Este proyecto implementa un ecosistema completo de Big Data. Procesa **100,000 registros** desde un almacenamiento NoSQL (**Cassandra**) hasta un Data Warehouse analÃ­tico (**ClickHouse**) usando **Apache Spark**.

---

## ðŸ› ï¸ Paso 1: InstalaciÃ³n de Herramientas (Si no tienes nada)

Si tu computadora estÃ¡ "limpia", debes instalar lo siguiente en este orden:

1. **Docker Desktop:** [Descargar aquÃ­](https://www.docker.com/products/docker-desktop/). Es el motor que correrÃ¡ las bases de datos.
2. **Python 3.10+:** [Descargar aquÃ­](https://www.python.org/downloads/). AsegÃºrate de marcar la casilla **"Add Python to PATH"** durante la instalaciÃ³n.
3. **Java JDK 11:** [Descargar aquÃ­](https://www.oracle.com/java/technologies/downloads/). Necesario para que Spark funcione.

---

## ðŸš€ Paso 2: ConfiguraciÃ³n del Proyecto

1. **Clonar el repositorio:** Descarga este proyecto como ZIP y extrÃ¡elo, o usa `git clone`.
2. **Levantar las Bases de Datos:** Abre una terminal (PowerShell o CMD) dentro de la carpeta del proyecto y ejecuta:
   ```bash
   docker-compose up -d
   ```
   Espera 1 minuto a que los motores arranquen por completo.
3. **Instalar conectores de Python:** En la misma terminal, ejecuta:
   ```bash
   pip install -r requirements.txt
   ```

---

## ðŸ“‘ Paso 3: EjecuciÃ³n del Pipeline (Jupyter)

Abre el archivo `.ipynb` con Jupyter Notebook o VS Code. Ejecuta todas las celdas en orden.

El script automÃ¡ticamente:
- CrearÃ¡ los datos en Cassandra.
- Los procesarÃ¡ con Spark.
- Los cargarÃ¡ refinados en ClickHouse.

---

## ðŸ“Š Paso 4: Â¿CÃ³mo ver las tablas resultantes?

Para verificar que todo funcionÃ³, usaremos la terminal para entrar a los contenedores y consultar las tablas:

### A. Ver Datos Crudos (Cassandra)

AquÃ­ estÃ¡n los 100,000 registros originales. Ejecuta en tu terminal:
```bash
docker exec -it cassandra_db cqlsh -e "SELECT * FROM proyecto_bigdata.ventas LIMIT 10;"
```

### B. Ver Resumen AnalÃ­tico (ClickHouse)

AquÃ­ verÃ¡s el resultado del procesamiento de Spark (Ventas totales por categorÃ­a). Ejecuta:
```bash
docker exec -it clickhouse_dw clickhouse-client -q "SELECT * FROM ventas_resumen ORDER BY total_ventas DESC FORMAT PrettyCompact;"
```

---

## ðŸ—ï¸ Resumen de la Arquitectura

- **Capa 1 (Ingesta):** Cassandra recibe los datos crudos (Escritura rÃ¡pida).
- **Capa 2 (Procesamiento):** Spark limpia duplicados y agrupa categorÃ­as.
- **Capa 3 (Servicio):** ClickHouse almacena el resumen para reportes (Lectura rÃ¡pida).

---

## âš ï¸ SoluciÃ³n de Errores Comunes

- **"Connection Refused":** Cassandra aÃºn estÃ¡ cargando. Espera 30 segundos y reintenta.
- **"Java not found":** Verifica que instalaste el JDK y reiniciaste tu terminal.
- **Docker lento:** AsegÃºrate de tener al menos 4GB de RAM asignados a Docker en Settings -> Resources.
