{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "136615e0-7f6b-435f-99f4-15752d634626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sesi√≥n de Spark inicializada con el conector de Cassandra.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, count, round\n",
    "\n",
    "# Detener sesiones previas para evitar conflictos de configuraci√≥n\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Inicializar sesi√≥n de Spark con el conector de Cassandra\n",
    "# Usamos el paquete oficial de Datastax para asegurar compatibilidad\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Pipeline_BigData_Fase3\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.0\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"cassandra_db\") \\\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\") \\\n",
    "    .config(\"spark.sql.extensions\", \"com.datastax.spark.connector.CassandraSparkExtensions\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Sesi√≥n de Spark inicializada con el conector de Cassandra.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81e43af2-dd80-493c-8dad-640241b0f7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Vista previa del resumen anal√≠tico (Fase 3 completada):\n",
      "+-------------------+-----------+--------------+----------------------+\n",
      "|        fecha_venta|  categoria|ventas_totales|cantidad_transacciones|\n",
      "+-------------------+-----------+--------------+----------------------+\n",
      "|2025-12-16 00:00:00|  Alimentos|     329990.06|                   656|\n",
      "|2026-02-03 00:00:00|       Ropa|     334491.98|                   678|\n",
      "|2026-01-21 00:00:00|  Alimentos|     331743.40|                   666|\n",
      "|2025-12-28 00:00:00|  Alimentos|     336338.33|                   642|\n",
      "|2026-01-09 00:00:00|   Deportes|     342667.70|                   690|\n",
      "|2025-12-28 00:00:00|   Deportes|     325962.41|                   640|\n",
      "|2026-01-25 00:00:00|Electr√≥nica|     339704.86|                   689|\n",
      "|2026-01-05 00:00:00|  Alimentos|     320445.81|                   642|\n",
      "|2026-02-03 00:00:00|Electr√≥nica|     369185.57|                   688|\n",
      "|2026-01-11 00:00:00|Electr√≥nica|     290856.38|                   593|\n",
      "+-------------------+-----------+--------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- fecha_venta: timestamp (nullable = true)\n",
      " |-- categoria: string (nullable = true)\n",
      " |-- ventas_totales: decimal(23,2) (nullable = true)\n",
      " |-- cantidad_transacciones: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Tarea 3.1: Lectura distribuida desde Cassandra ---\n",
    "df_crudo = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"ventas_crudas\", keyspace=\"proyecto_bigdata\") \\\n",
    "    .load()\n",
    "\n",
    "# --- Tarea 3.3: Limpieza de datos ---\n",
    "# Filtramos montos negativos o nulos para asegurar la calidad del an√°lisis\n",
    "df_limpio = df_crudo.filter(col(\"monto_total\") > 0)\n",
    "\n",
    "# --- Tarea 3.2: L√≥gica de Transformaci√≥n y Agregaci√≥n ---\n",
    "# Agrupamos por fecha y categor√≠a para obtener totales y volumen de transacciones\n",
    "df_resumen = df_limpio.groupBy(\"fecha_venta\", \"categoria\") \\\n",
    "    .agg(\n",
    "        round(sum(\"monto_total\"), 2).alias(\"ventas_totales\"),\n",
    "        count(\"id_venta\").alias(\"cantidad_transacciones\")\n",
    "    )\n",
    "\n",
    "print(\"üìä Vista previa del resumen anal√≠tico (Fase 3 completada):\")\n",
    "df_resumen.show(10)\n",
    "df_resumen.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40aab025-b5ea-4c6e-b425-f93a7384eba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: clickhouse-connect in /opt/conda/lib/python3.11/site-packages (0.10.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from clickhouse-connect) (2023.7.22)\n",
      "Requirement already satisfied: urllib3>=1.26 in /opt/conda/lib/python3.11/site-packages (from clickhouse-connect) (2.0.7)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.11/site-packages (from clickhouse-connect) (2023.3.post1)\n",
      "Requirement already satisfied: zstandard in /opt/conda/lib/python3.11/site-packages (from clickhouse-connect) (0.21.0)\n",
      "Requirement already satisfied: lz4 in /opt/conda/lib/python3.11/site-packages (from clickhouse-connect) (4.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install clickhouse-connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "695ca0b4-9e76-4399-baba-b8e9a4b87fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Extrayendo datos procesados de Spark...\n",
      "‚úÖ ¬°MISI√ìN CUMPLIDA! Se cargaron 300 filas en ClickHouse.\n",
      "‚ú® El Pipeline Cassandra -> Spark -> ClickHouse est√° 100% operativo.\n"
     ]
    }
   ],
   "source": [
    "# --- FASE 4.1: CARGA AL DATA WAREHOUSE \n",
    "import clickhouse_connect\n",
    "\n",
    "print(\"üì¶ Extrayendo datos procesados de Spark...\")\n",
    "# Preparar los registros\n",
    "registros = [list(row) for row in df_resumen.collect()]\n",
    "\n",
    "try:\n",
    "    # Intentamos conexi√≥n al usuario 'default' que acabamos de resetear\n",
    "    client = clickhouse_connect.get_client(\n",
    "        host='172.18.0.3', \n",
    "        port=8123, \n",
    "        username='default', \n",
    "        password=''\n",
    "    )\n",
    "\n",
    "    # 1. Aseguramos que la base de datos y la tabla existan\n",
    "    client.command(\"CREATE DATABASE IF NOT EXISTS default\")\n",
    "    \n",
    "    client.command(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS default.ventas_resumen (\n",
    "            fecha_venta Date,\n",
    "            categoria String,\n",
    "            ventas_totales Float64,\n",
    "            cantidad_transacciones Int64\n",
    "        ) ENGINE = MergeTree() ORDER BY (fecha_venta, categoria)\n",
    "    \"\"\")\n",
    "\n",
    "    # 2. Insertamos los datos de manera masiva\n",
    "    client.insert('default.ventas_resumen', registros, \n",
    "                  column_names=['fecha_venta', 'categoria', 'ventas_totales', 'cantidad_transacciones'])\n",
    "\n",
    "    print(f\"‚úÖ ¬°MISI√ìN CUMPLIDA! Se cargaron {len(registros)} filas en ClickHouse.\")\n",
    "    print(\"‚ú® El Pipeline Cassandra -> Spark -> ClickHouse est√° 100% operativo.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error de conexi√≥n: {e}\")\n",
    "    print(\"üí° Si persiste el error de password, intenta cambiar username='default' por username='admin' en la l√≠nea 12.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8357e5a3-9125-43ec-83c1-ea0a247df8de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
